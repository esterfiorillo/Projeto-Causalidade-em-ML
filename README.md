<div align="center">

# Projeto Causalidade em Aprendizado de M√°quina

[![python](https://img.shields.io/badge/-Python_3.7_%7C_3.8_%7C_3.9_%7C_3.10-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![pytorch](https://img.shields.io/badge/PyTorch_1.8+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)

<!-- <a href="https://www.python.org/"><img alt="Python" src="https://img.shields.io/badge/-Python 3.7+-blue?style=for-the-badge&logo=python&logoColor=white"></a> -->

<!-- <a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/-PyTorch 1.8+-ee4c2c?style=for-the-badge&logo=pytorch&logoColor=white"></a>
<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning 1.6+-792ee5?style=for-the-badge&logo=pytorchlightning&logoColor=white"></a>
<a href="https://hydra.cc/"><img alt="Config: hydra" src="https://img.shields.io/badge/config-hydra 1.2-89b8cd?style=for-the-badge&labelColor=gray"></a>
<a href="https://black.readthedocs.io/en/stable/"><img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-black.svg?style=for-the-badge&labelColor=gray"></a> -->


</div>

<br>

## Membros

- Ester Fiorillo 
- Rodrigo Andrade

## üìå¬†¬†Introduction

O aprendizado profundo (Deep Learnign - DL) √© um dos ramos da intelig√™ncia artificial que teve um crescimento exponencial nos √∫ltimos anos. A comunidade cient√≠fica tem focado sua aten√ß√£o na DL devido √† sua versatilidade, alto desempenho, alta capacidade de generaliza√ß√£o, usos multidisciplinares, entre muitas outras qualidades. Al√©m disso, uma grande quantidade de dados m√©dicos se tornaram acess√≠veis e o desenvolvimento de computadores mais potentes tamb√©m fomentaram o interesse na √°rea de IA aplicado √† dados m√©dicos.

A radiografia de t√≥rax √© o exame de imagem mais comum em todo o mundo, cr√≠tico para triagem, diagn√≥stico e tratamento de muitas doen√ßas potencialmente fatais. Interpreta√ß√£o automatizada de radiografia de t√≥rax no n√≠vel de radiologistas praticantes pode fornecer benef√≠cios substanciais em muitos casos m√©dicos, como por exemplo na melhoria na prioriza√ß√£o de pacientes, decis√£o cl√≠nica, apoio √† triagem em larga escala e elabora√ß√£o de iniciativas de sa√∫de da popula√ß√£o global.

## Dataset - CheXpert

O CheXpert √© um grande conjunto de dados de radiografias de t√≥rax (224.316 radiografias de 65.240 pacientes) coletado entre outrubro de 2002 e julho de 2017, no hospital de Stanford. O dataset foi interpretado para presen√ßa de 14  tipos de enfermidades. As interpreta√ß√µes foram feitas inicialmente pelos relat√≥rios de avalia√ß√£o e em seguida pelo consenso de um grupo de 3 radiologiastas.

Os dados de valida√ß√£o consistem em 200 amostras selecionadas aleatoriamente no dataset. O dataset de teste consiste em 500 casos de 500 pacientes in√©ditos aos dados de treino e valida√ß√£o. Oito radiologistas certificados pelo conselho de medicina americano anotaram individualmente cada um dos estudos no conjunto de testes, classificando cada enfermidade em: (1) presente, (2) prov√°vel incerto,  (3) improv√°vel incerto e (4) ausente. Suas anota√ß√µes foram binarizadas de forma que todos os casos prov√°veis presentes e incertos sejam tratados como positivos e todos os casos improv√°veis incertos e ausentes sejam tratados como negativos. A maioria dos votos de 5 anota√ß√µes do radiologista serviu como uma anota√ß√£o VERDADE; as 3 anota√ß√µes restantes do radiologista foram usadas para avaliar o desempenho dos radiologistas.

Al√©m do dataset os autores experimentaram v√°rias arquiteturas de rede neural convolucional, especificamente ResNet152, DenseNet121, Inception-v4 e SEResNeXt101, e descobriu que a arquitetura DenseNet121 produziu os melhores resultados. Este classificador pode ser utilizado nos estudos de causalidade (discutido a seguir). Os autores treinaram modelos que tomam como entrada uma radiografia de t√≥rax de vis√£o √∫nica e emitem a probabilidade de cada uma das 14 observa√ß√µes. Quando mais de uma visualiza√ß√£o estiver dispon√≠vel, os modelos retornaram a probabilidade m√°xima das observa√ß√µes ao longo as visualiza√ß√µes.

## Modelo de DL - Classificador CheXnet

A Chexnet √© um modelo baseado em uma rede neural artificial convolucional de 121 layers, feita para a tarefa da classifica√ß√£o multilabel, ou seja, uma mesma amostra pode possuir mais de uma classe. No caso do problema da classifica√ß√£o de doen√ßas pulmonares em imagens de Raio-X tor√°cicas e do dataset CheXpert escolhido, a Chexnet poder√° fazer a predi√ß√£o de uma imagem em at√© 13 classes de doen√ßas.

A rede foi originalmente treinada em outro dataset de imagens de Raio-X tor√°cicas, o Chest-X-Ray 14. Ele tamb√©m √© considerado um conjunto de dados para a tarefa de classifica√ß√£o de doen√ßas multilabel. Para o trabalho proposto foi feita a escolha de retreinar o modelo usando outro dataset, no caso o Chexpert, devido a esse ter sido lan√ßado alguns anos depois e possuir ainda mais amostras que o Chest-X-Ray 14.

Mais detalhes sobre a rede neural que ser√° utilizada incluem dizer que ela consiste em uma rede densa, logo ela possui um bom desempenho em passar fluxos de informa√ß√µes e gradientes atrav√©s do modelo, sendo assim sua otimiza√ß√£o mais trat√°vel. Outro ponto importante a respeito da arquitetura √© que seu √∫ltimo layer completamente conectado foi substitu√≠do por um que possui apenas um output e depois foi feita uma opera√ß√£o sigmoid n√£o linear.

Por fim, a respeito da metodologia de treino, √© pretendido que para experimentos iniciais o protocolo de treino seja semelhante ao apresentado no paper da Chexnet. Portanto, os pesos da Chexnet ser√£p inicializados com os pesos de um modelo pr√© treinado no ImageNet e a rede ser√° treinada end-to-end com o otimizador Adam, uma taxa de aprendizado de 0,001 e em mini batchs de tamanho 16.

## M√©tricas

Como o problema em quest√£o trata-se de uma classifica√ß√£o multilabel, a m√©trica utilizada nesse trabalho ser√° a predominante na literatura para esse tipo de problema. No caso, ser√° usada a √°rea abaixo da curva ROC (AUROC). Uma curva ROC mostra a compensa√ß√£o entre a taxa de verdadeiros positivos (TPR) e a taxa de falsos positivos (FPR) em diferentes limites de decis√£o.

AUROC portanto √© uma m√©trica de desempenho para ‚Äúdiscrimina√ß√£o‚Äù: informa sobre a capacidade do modelo de discriminar entre casos (exemplos positivos) e n√£o casos (exemplos negativos).

## Explicabilidade

T√©cnicas de Aprendizado de M√°quina se tornaram extremamente boas em generalizar dados independentes e com distribui√ß√£o semelhante. No caso das imagens de raio X e do problema da classifica√ß√£o de doen√ßas nessas imagens, os modelos estado da arte atingiram m√©tricas que chegam a ultrapassar a performance de profissionais da sa√∫da na mesma tarefa. 

Entretanto, h√° uma grande barreira no uso efetivo desses algoritmos na vida pr√°tica, especialmente quando envolve a √°rea da sa√∫de. A causa dessa barreira se d√° pela falta de explicabilidade das predi√ß√µes dos modelos, ou seja, h√° uma grande dificuldade em saber os motivos pelos quais aquele modelo disse que um exame de raio X possui o diagn√≥stico de uma enfermidade. Essa falta de explica√ß√µes gera uma falta de confian√ßa nos modelos, fato que faz muitas pessoas preferirem diagn√≥sticos feitos por m√©dicos e n√£o por m√°quinas, mesmo sabendo que a m√°quina pode ter mais chances de acertar.

Nesse contexto, surgiu a √°rea de explicabilidade em Aprendizado de M√°quina e ela tem como objetivo expor o porque de alguma predi√ß√£o ter sido feita por algum modelo. Uma forma de fazer isso √© por meio de m√©todos chamados atribui√ß√£o de features, que fazem um ranking de features, ou seja, representam as contribui√ß√µes de cada feature para o output do modelo. Nesse trabalho, foram pesquisadas duas ferramentas que visam realizar essa tarefa de demonstrar quais features fizeram mais diferen√ßa para que um modelo chegasse a uma predi√ß√£o. Ambas ser√£o utilizadas junto a CheXnet, de forma a localizar pixels da imagem que foram mais relevantes para que a rede tenha classificado alguma imagem de Raio-X com alguma doen√ßa. Mais detalhe sobre cada uma dessas ferramentas se encontram nas subse√ß√µes abaixo.

## CXPlain

O CXPlain √© um m√©todo de explica√ß√£o em que √© treinado separadamente um modelo supervisionado para explica√ß√£o causal do modelo de predi√ß√£o. S√£o usadas fun√ß√µes de influ√™ncia causal para quantificar a influencia das features e de grupos de features na acur√°cia do modelo.

Para cada feature, √© calculado o fator Ai que denota a influencia causal de adcionar aquela feature ao conjunto de features de entrada. Por fim, tamb√©m s√£o fornecidos intervalos de confian√ßa que estimam o n√≠vel de incerteza associada a importancia de cada feature.

## LIME

LIME √© uma biblioteca que se prop√µem a fornecer uma implementa√ß√£o concreta de modelos explicativos locais. Os c√≥digos est√£o dispon√≠veis em https://github.com/marcotcr/lime. 

Conforme descrito em \citet*{Molnar}, a ideia √© bastante intuitiva. Primeiro, esque√ßa os dados de treinamento e imagine que voc√™ tem apenas o classificar onde pode inserir pontos de dados e obter as previs√µes do modelo. O LIME testa o que acontece com as previs√µes quando voc√™ fornece varia√ß√µes de seus dados para o classificador. A biblioteca gera um novo conjunto de dados que consiste em amostras perturbadas e as previs√µes correspondentes do Classificador. Nesse novo conjunto de dados, o LIME treina um modelo interpret√°vel, que √© ponderado pela proximidade das inst√¢ncias amostradas √† inst√¢ncia de interesse. O modelo aprendido deve ser uma boa aproxima√ß√£o das previs√µes do modelo de aprendizado de m√°quina localmente, mas n√£o precisa ser uma boa aproxima√ß√£o global. Esse tipo de precis√£o tamb√©m √© chamado de fidelidade local.

Matematicamente, modelos explicativos locais com restri√ß√£o de interpretabilidade podem ser expressos da seguinte forma:

\begin{equation}
explic(x) = \underset{g \ \in \ G}{argmin} \ L(f,g,\pi_(x) +\omega(g)
\end{equation}

Onde $L$ √© a fun√ß√£o de custo, $x$ √© a inst√¢ncia local, $f$ √© o classificador original, $g$ √© o modelo explicativo,  G √© a fam√≠lia de poss√≠veis explica√ß√µes e $\pi_(x)$ √© uma m√©trica de dist√¢ncia da amostra original para a amostra gerada pelo LIME.

O passo-a-passo para treinar o modelo explicativo local √© : 
\begin{itemize}
\item Selecione sua inst√¢ncia de interesse para a qual voc√™ deseja ter uma explica√ß√£o da previs√£o do seu classificador . 
\item Perturbe seu conjunto de dados e obtenha as previs√µes do classificador para esses novos pontos. 
\item Ponderar as novas amostras de acordo com sua proximidade com a inst√¢ncia de interesse.
\item Treine um modelo ponderado e interpret√°vel no conjunto de dados com as varia√ß√µes. 
\item Explique a previs√£o interpretando o modelo local.
\end{itemize}

Pretendemos comparar esta biblioteca com a performance e explica√ß√µes do CXPlain.

## Valida√ß√£o com profissional da sa√∫de

Ap√≥s treinar o modelo de classifica√ß√£o de doen√ßas pulmonares Chexnet e utilizar as ferramentas de explicabilidade Lime e CXPlain, teremos as predi√ß√µes obtidas pelo modelo e mapas que indicam quais features foram mais relevantes para essas predi√ß√µes terem sido feitas. 

No caso, essas ferramentas nos retornam regi√µes da imagem que tiveram mais relevancia para o modelo. Com esses recursos, espera-se que o modelo tenha utilizado de informa√ß√µes como textura, tamanho, presen√ßa de n√≥dulos, entre outras caracter√≠sticas dos pulm√µes para predizer alguma doen√ßa. Nesse sentido e tendo esses recursos em m√£os, a √∫ltima etapa do trabalho consistir√° em mostrar esses recursos para uma m√©dica que o grupo tem contato e pedir uma an√°lise dos mapas de relevancia apontados pelo lime e pelo CXPlain. O objetivo √© e fato determinar se a rede utilizou elementos consistentes dos pulm√µes para determinar as classes de doen√ßas.
